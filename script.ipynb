from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import DateType
# Initialize Spark session
spark = SparkSession.builder \
    .appName("Convert_Excel_to_CSV_Fabric") \
    .config("spark.jars.packages", "com.crealytics:spark-excel_2.12:0.13.5") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
    .getOrCreate()


# Define root folder where Excel files are stored
source_folder = '/lakehouse/default/Files/COMMERCIAL_INGEST/RAW/'

excel_path = '/lakehouse/default/Files/COMMERCIAL_INGEST/RAW/inventory data.xlsx'
# Output folder to save converted CSVs
output_folder= '/lakehouse/default/Files/COMMERCIAL_INGEST/OUTPUT/'

base_path = "Files/COMMERCIAL_INGEST/OUTPUT/"

import os
import re
import pandas as pd
# Files to exclude from skiprows=11
excluded_prefixes = ("costing", "sku")

# Loop through all Excel files in the RAW folder
for file in os.listdir(source_folder):
    if file.endswith(".xls"):
        source_path = os.path.join(source_folder, file)
        output_filename = os.path.splitext(file)[0] + ".csv"
        output_path = os.path.join(output_folder, output_filename)

        try:
            # Apply skiprows only if not in excluded list
            if file.lower().startswith(excluded_prefixes):
                df = pd.read_excel(source_path)
            else:
                df = pd.read_excel(source_path, skiprows=11)

            # Replace \n and \r only in string columns to avoid CSV formatting issues
            for column in df.select_dtypes(include=['object']).columns:
                df[column] = df[column].map(lambda x: x.replace('\n', ' ').replace('\r', ' ') if isinstance(x, str) else x)

            # Save to CSV
            df.to_csv(output_path, index=False)
            print(f"✅ Converted: {file} → {output_filename}")

        except Exception as e:
            print(f"❌ Failed to convert {file}: {e}")


import pandas as pd

# Load sheets
fabric_df = pd.read_excel(excel_path, sheet_name="fabric")
trim_df = pd.read_excel(excel_path, sheet_name="trim")

# --- Handle "trim code" sheet with extra header row ---
trim_code_df_raw = pd.read_excel(excel_path, sheet_name="trim code", header=None)
trim_code_df_raw = trim_code_df_raw.drop(index=1)  # Drop second row
trim_code_df_raw.columns = trim_code_df_raw.iloc[0]  # Promote first row as headers
trim_code_df = trim_code_df_raw.drop(index=0).reset_index(drop=True)

# --- Clean and transform fabric sheet ---
fabric_df.columns = fabric_df.columns.str.strip()
fabric_df["Check"] = fabric_df["Fabric code"].apply(
    lambda x: "ERP" if any(keyword in str(x) for keyword in ["SS1", "SB1", "S1-"]) else "NO ERP"
)
fabric_df_filtered = fabric_df[fabric_df["Check"] == "ERP"].copy()

# --- Clean and transform trim code sheet ---
trim_code_df.columns = trim_code_df.columns.str.strip()
trim_code_df["DATE"] = pd.to_datetime(trim_code_df["DATE"], errors='coerce')
trim_code_df["Year"] = trim_code_df["DATE"].dt.year
trim_code_df_filtered = trim_code_df[trim_code_df["Year"] == 2024].copy()
trim_code_df_filtered["Check"] = trim_code_df_filtered["Customer"].apply(
    lambda x: "ERP" if any(keyword in str(x) for keyword in ["SS2C00", "SB2C00", "S2-24"]) else "NO ERP"
)

# --- Clean and transform trim sheet ---
trim_df.columns = trim_df.columns.str.strip()
trim_df_filtered = trim_df[trim_df["Note"] == "Trims- mainline"].copy()

# --- Merge trim with trim code ---
merged_trim = pd.merge(
    trim_df_filtered,
    trim_code_df_filtered[["Fabric Code", "Check"]],
    how="left",
    left_on="Mã vật tư",
    right_on="Fabric Code"
)
merged_trim = merged_trim.dropna(subset=["Check"])
merged_trim_filtered = merged_trim[merged_trim["Check"] == "ERP"].copy()

# --- Add standardized customer name ---
def map_customer(cus_name):
    if cus_name == "AIME LEON DORE":
        return "ALD"

    else:
        return cus_name

merged_trim_filtered["Customer"] = merged_trim_filtered["Cus. Name"].apply(map_customer)



# CREATE FUNCTION 
# Helper function to clean & transform each DataFrame
def clean_fabric_df(path, kind_of_request):
    df = spark.read.format("csv") \
        .option("header", "true") \
        .load(path)
    
    # Clean column names: strip spaces, replace illegal characters
    for old_col in df.columns:
        new_col = old_col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace("#", "")
        df = df.withColumnRenamed(old_col, new_col)

    # Fix Sent_Date format if it exists
    if "Sent_Date" in df.columns:
        df = df.withColumn("Sent_Date_Split", split(col("Sent_Date"), " ")) \
               .withColumn("Sent_Date_Main", col("Sent_Date_Split")[0]) \
               .withColumn("Sent_Date_Formatted", split(col("Sent_Date_Main"), "/")) \
               .withColumn("Sent_Date", concat_ws("/", col("Sent_Date_Formatted")[1], col("Sent_Date_Formatted")[0], col("Sent_Date_Formatted")[2])) \
               .drop("Sent_Date_Split", "Sent_Date_Main", "Sent_Date_Formatted") \
               .withColumn("Sent_Date", col("Sent_Date").cast(DateType()))

    # Filter only finished requests
    # if "Status" in df.columns:
    #     df = df.filter(trim(col("Status")) == "Finished")

    # Add kind of request
    df = df.withColumn("Kind_of_request", lit(kind_of_request))

    return df

# Clean individual datasets
df_fabric_dev = clean_fabric_df(base_path + "managefabricdevelopments.csv", "Fabric dev")
df_fabric_labdip = clean_fabric_df(base_path + "managefabriclabdip.csv", "Fabric labdip")
df_fabric_swatch = clean_fabric_df(base_path + "managefabricswatchs.csv", "Fabric swatch")

# Union all three
df_fabric_requests = df_fabric_dev.unionByName(df_fabric_labdip, allowMissingColumns=True) \
                    .unionByName(df_fabric_swatch, allowMissingColumns=True)


# Show final combined result
# display(df_fabric_requests)

# Reusable cleaning function
def clean_trim_df(path, kind_of_request, finished_column="Status"):
    df = spark.read.format("csv") \
        .option("header", "true") \
        .load(path)
    
    # Sanitize column names
    for old_col in df.columns:
        new_col = old_col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace("#", "").replace("/", "_")
        df = df.withColumnRenamed(old_col, new_col)

    # Fix Sent_Date if exists
    if "Sent_Date" in df.columns:
        df = df.withColumn("Sent_Date_Split", split(col("Sent_Date"), " ")) \
               .withColumn("Sent_Date_Main", col("Sent_Date_Split")[0]) \
               .withColumn("Sent_Date_Formatted", split(col("Sent_Date_Main"), "/")) \
               .withColumn("Sent_Date", concat_ws("/", col("Sent_Date_Formatted")[1], col("Sent_Date_Formatted")[0], col("Sent_Date_Formatted")[2])) \
               .drop("Sent_Date_Split", "Sent_Date_Main", "Sent_Date_Formatted") \
               .withColumn("Sent_Date", col("Sent_Date").cast(DateType()))

    # Filter only finished rows
    # if finished_column in df.columns:
    #     df = df.filter(trim(col(finished_column)) == "Finished")

    # Add kind of request
    df = df.withColumn("Kind_of_request", lit(kind_of_request))

    return df

# Load and clean each trim dataset
df_trim_dev = clean_trim_df(base_path +"managetrimdevelop.csv", "Trim dev")
df_trim_labdip = clean_trim_df(base_path +"managetrimlabdip.csv", "Trim labdip", finished_column="Pur_Status")
df_trim_swatch = clean_trim_df(base_path +"managetrimswatch.csv", "Trim swatch")

# Combine into one DataFrame (UNION ALL with missing column support)
df_trim_requests = df_trim_dev.unionByName(df_trim_labdip, allowMissingColumns=True) \
                              .unionByName(df_trim_swatch, allowMissingColumns=True)

# Show result
# display(df_trim_requests)



# === Universal cleaner for tech requests ===
def clean_tech_df(path, kind_of_request, date_col="Sent_Date"):
    df = spark.read.format("csv").option("header", "true").load(path)

    # Clean column names
    for old_col in df.columns:
        new_col = old_col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace("#", "").replace("/", "_")
        df = df.withColumnRenamed(old_col, new_col)

    # Fix Sent Date format
    if date_col in df.columns:
        df = df.withColumn(f"{date_col}_Split", split(col(date_col), " ")) \
               .withColumn(f"{date_col}_Main", col(f"{date_col}_Split")[0]) \
               .withColumn(f"{date_col}_Parts", split(col(f"{date_col}_Main"), "/")) \
               .withColumn("Sent_Date", concat_ws("/", col(f"{date_col}_Parts")[1], col(f"{date_col}_Parts")[0], col(f"{date_col}_Parts")[2])) \
               .drop(f"{date_col}_Split", f"{date_col}_Main", f"{date_col}_Parts") \
               .withColumn("Sent_Date", col("Sent_Date").cast(DateType()))

    # Add kind of request column
    df = df.withColumn("Kind_of_request", lit(kind_of_request))

    return df

# === Load and clean all 4 tech datasets ===
df_fabric_rating = clean_tech_df(base_path +"managefabrictechnicals.csv", "Fabric rating")
df_trim_rating   = clean_tech_df(base_path +"managetrimtechnicals.csv", "Trim rating")
df_cmp           = clean_tech_df(base_path +"managecmps.csv", "CMP", date_col="Sent_Technical_Date")
df_pom           = clean_tech_df(base_path +"pom-request.csv", "POM", date_col="Sent_Technical_Date")

# === Combine all ===
df_tech_requests = df_fabric_rating.unionByName(df_trim_rating, allowMissingColumns=True) \
                                   .unionByName(df_cmp, allowMissingColumns=True) \
                                   .unionByName(df_pom, allowMissingColumns=True)

# === Show result ===
# display(df_tech_requests)


# === Universal cleaner for treatment requests ===
def clean_treatment_df(path, kind_of_request, date_col="Sent_Date"):
    df = spark.read.format("csv").option("header", "true").load(path)

    # Standardize column names
    for old_col in df.columns:
        new_col = old_col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace("#", "").replace("/", "_")
        df = df.withColumnRenamed(old_col, new_col)

    # Fix and cast Sent Date
    if date_col in df.columns:
        df = df.withColumn(f"{date_col}_Split", split(col(date_col), " ")) \
               .withColumn(f"{date_col}_Main", col(f"{date_col}_Split")[0]) \
               .withColumn(f"{date_col}_Parts", split(col(f"{date_col}_Main"), "/")) \
               .withColumn("Sent_Date", concat_ws("/", col(f"{date_col}_Parts")[1], col(f"{date_col}_Parts")[0], col(f"{date_col}_Parts")[2])) \
               .drop(f"{date_col}_Split", f"{date_col}_Main", f"{date_col}_Parts") \
               .withColumn("Sent_Date", col("Sent_Date").cast(DateType()))

    # Add kind_of_request label
    df = df.withColumn("Kind_of_request", lit(kind_of_request))
    
    return df

df_printing        = clean_treatment_df(base_path +"manageprintings.csv", "Printing inhouse")
df_prt_outsource   = clean_treatment_df(base_path +"manageprintoutsources.csv", "Printing outsource")
df_emb             = clean_treatment_df(base_path +"manageemps.csv", "EMB")
df_dyewash         = clean_treatment_df(base_path +"managedyewashs.csv", "Dyewash")
# Combine all
df_treatment_requests = df_printing.unionByName(df_prt_outsource, allowMissingColumns=True) \
                                   .unionByName(df_emb, allowMissingColumns=True) \
                                   .unionByName(df_dyewash, allowMissingColumns=True)
# === Show result ===
# display(df_treatment_requests)  # Or df_treatment_requests.show()


# Universal cleaner
def clean_csv_df(path, kind_of_request=None, date_col="Created_Date", convert_datetime=True):
    df = spark.read.option("header", "true").csv(path)

    # Clean column names
    for old_col in df.columns:
        new_col = old_col.strip().replace(" ", "_").replace("(", "").replace(")", "").replace("#", "").replace("/", "_")
        df = df.withColumnRenamed(old_col, new_col)

    # Handle date column
    if date_col in df.columns:
        df = df.withColumn(f"{date_col}_Split", split(col(date_col), " ")) \
               .withColumn(f"{date_col}_Main", col(f"{date_col}_Split")[0]) \
               .withColumn(f"{date_col}_Parts", split(col(f"{date_col}_Main"), "/")) \
               .withColumn(date_col, concat_ws("/", col(f"{date_col}_Parts")[1], col(f"{date_col}_Parts")[0], col(f"{date_col}_Parts")[2]))

        if convert_datetime:
            df = df.withColumn(date_col, col(date_col).cast(DateType()))

        df = df.drop(f"{date_col}_Split", f"{date_col}_Main", f"{date_col}_Parts")

    if kind_of_request:
        df = df.withColumn("Kind_of_request", lit(kind_of_request))

    return df


    # Define all your file paths
    #df_sku
df_style_order     = clean_csv_df(base_path + "manageliststyleoforder.csv", kind_of_request="Style Order")
df_po              = clean_csv_df(base_path + "managepurchaseorder.csv", kind_of_request="Purchase Order")
df_sku             = clean_csv_df(base_path + "sku.csv")
df_users           = clean_csv_df(base_path + "danh-sach-nguoi-dungcong-ty-unavailable.csv", kind_of_request="User List", convert_datetime=False)
df_season          = clean_csv_df(base_path + "manageseason.csv", kind_of_request="Season", convert_datetime=False)
df_customers       = clean_csv_df(base_path + "managecustomers.csv", kind_of_request="Customer", convert_datetime=False)
df_style_category  = clean_csv_df(base_path + "managestylecategory.csv", kind_of_request="Style Category", convert_datetime=False)

